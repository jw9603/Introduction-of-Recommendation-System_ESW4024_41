{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가즈아!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "'''\n",
    "# input\n",
    "#    - pred_u: 예측 값으로 정렬 된 item index\n",
    "#    - target_u: test set의 item index\n",
    "#    - top_k: top-k에서의 k 값\n",
    "'''\n",
    "def compute_metrics(pred_u, target_u, top_k):\n",
    "    pred_k = pred_u[:top_k]\n",
    "    num_target_items = len(target_u)\n",
    "\n",
    "    hits_k = [(i + 1, item) for i, item in enumerate(pred_k) if item in target_u]\n",
    "    num_hits = len(hits_k)\n",
    "\n",
    "    idcg_k = 0.0\n",
    "    for i in range(1, min(num_target_items, top_k) + 1):\n",
    "        idcg_k += 1 / math.log(i + 1, 2)\n",
    "\n",
    "    dcg_k = 0.0\n",
    "    for idx, item in hits_k:\n",
    "        dcg_k += 1 / math.log(idx + 1, 2)\n",
    "\n",
    "    prec_k = num_hits / top_k\n",
    "    recall_k = num_hits / min(num_target_items, top_k)\n",
    "    ndcg_k = dcg_k / idcg_k\n",
    "\n",
    "\n",
    "    return prec_k, recall_k, ndcg_k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_csr_matrix(data: pd.DataFrame, shape: tuple):\n",
    "    row = data['user_id'].tolist()\n",
    "    col = data['item_id'].tolist()\n",
    "    data = data['rating'].tolist()\n",
    "\n",
    "    return sparse.csr_matrix((data, (row, col)), shape=shape)\n",
    "\n",
    "def load_data(data_path, implicit=True):\n",
    "    train_data_path = f'{data_path}/train_data.csv'\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    valid_data_path = f'{data_path}/valid_data.csv'\n",
    "    valid_data = pd.read_csv(valid_data_path)\n",
    "    test_data_path = f'{data_path}/test_data.csv'\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    # train_data = pd.concat([train_data, valid_data], axis=0)\n",
    "    train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "    valid_data = valid_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    num_items = max([train_data['item_id'].max(), test_data['item_id'].max(), valid_data['item_id'].max()]) + 1\n",
    "\n",
    "    num_users_train = train_data['user_id'].unique().shape[0]\n",
    "    train_matrix = get_csr_matrix(train_data, shape=(num_users_train, num_items))\n",
    "\n",
    "    num_users_valid = valid_data['user_id'].unique().shape[0]\n",
    "    valid_input, valid_target = train_test_split(valid_data, test_size=0.2, stratify=valid_data['user_id'], random_state=506)\n",
    "    valid_matrix_input = get_csr_matrix(valid_input, shape=(num_users_valid, num_items))\n",
    "    valid_matrix_target = get_csr_matrix(valid_target, shape=(num_users_valid, num_items))\n",
    "\n",
    "    num_users_test = test_data['user_id'].unique().shape[0]\n",
    "    test_input, test_target = train_test_split(test_data, test_size=0.2, stratify=test_data['user_id'], random_state=506)\n",
    "    test_matrix_input = get_csr_matrix(test_input, shape=(num_users_test, num_items))\n",
    "    test_matrix_target = get_csr_matrix(test_target, shape=(num_users_test, num_items))\n",
    "\n",
    "    num_train_users = train_matrix.shape[0]\n",
    "    num_valid_users = valid_matrix_input.shape[0]\n",
    "    num_test_users = test_matrix_input.shape[0]\n",
    "    num_total_users = num_train_users + num_valid_users + num_test_users\n",
    "\n",
    "    # num_items = train_matrix.shape[1] # 이게 사용됨\n",
    "    train_valid = sparse.vstack([train_matrix, (valid_matrix_input+valid_matrix_target)]).toarray()\n",
    "    rating_cnt = train_valid.sum(axis=0)\n",
    "\n",
    "    print(\"############################################\")\n",
    "    print(f\"# of users: {num_total_users}\")\n",
    "    print(f\"# of items: {num_items}\")\n",
    "    print(f\"# of train users (ratings): {num_train_users} ({train_matrix.nnz})\")\n",
    "    print(f\"# of valid users (# of input ratings, # of target ratings): {num_valid_users} ({valid_matrix_input.nnz}, {valid_matrix_target.nnz})\")\n",
    "    print(f\"# of test users (# of input ratings, # of target ratings): {num_test_users} ({test_matrix_input.nnz}, {test_matrix_target.nnz})\")\n",
    "    print(\"############################################\")\n",
    "\n",
    "    if implicit:\n",
    "        train_matrix.data[:] = 1\n",
    "        valid_matrix_input.data[:] = 1\n",
    "        valid_matrix_target.data[:] = 1\n",
    "        test_matrix_input.data[:] = 1\n",
    "        test_matrix_target.data[:] = 1\n",
    "\n",
    "    return train_matrix.toarray(), valid_matrix_input.toarray(), valid_matrix_target.toarray(), test_matrix_input.toarray(), test_matrix_target.toarray(), train_data, valid_data,test_data, train_valid, rating_cnt, num_items\n",
    "\n",
    "def eval_implicit(model, eval_input, eval_target, top_k):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        prec_list = []\n",
    "        recall_list = []\n",
    "        ndcg_list = []\n",
    "        \n",
    "        eval_indices = np.arange(eval_input.shape[0])\n",
    "        eval_loader = DataLoader(eval_indices, batch_size=model.batch_size)\n",
    "        \n",
    "        for batch_indices in eval_loader:\n",
    "            batch_data = torch.FloatTensor(eval_input[batch_indices]).to(model.device)\n",
    "            preds = model.forward(batch_data).cpu().numpy()\n",
    "            for i, u_idx in enumerate(batch_indices):\n",
    "                input_by_user = eval_input[u_idx]\n",
    "                missing_item_ids = np.where(input_by_user == 0)[0]\n",
    "                pred_u_score = preds[i][missing_item_ids]\n",
    "                pred_u_idx = np.argsort(pred_u_score)[::-1]  # 내림차순 정렬\n",
    "                pred_u = missing_item_ids[pred_u_idx]\n",
    "\n",
    "                target_by_user = eval_target[u_idx]\n",
    "                target_u = np.where(target_by_user >= 0.5)[0]\n",
    "                \n",
    "                prec_k, recall_k, ndcg_k = compute_metrics(pred_u, target_u, top_k)\n",
    "                prec_list.append(prec_k)\n",
    "                recall_list.append(recall_k)\n",
    "                ndcg_list.append(ndcg_k)\n",
    "\n",
    "    return np.mean(prec_list), np.mean(recall_list), np.mean(ndcg_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "1. AE\n",
    "2. DAE\n",
    "3. MultiVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from IPython import embed\n",
    "\n",
    "from time import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from IPython import embed\n",
    "\n",
    "from time import time\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython import embed\n",
    "\n",
    "from time import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DAE_implicit(torch.nn.Module):\n",
    "    def __init__(self, train_matrix, valid_matrix_input, valid_matrix_target, batch_size, max_epochs, hidden_dim, learning_rate, reg_lambda, dropout, eval_topk,seed, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.train_matrix = train_matrix\n",
    "        self.valid_matrix_input = valid_matrix_input\n",
    "        self.valid_matrix_target = valid_matrix_target\n",
    "\n",
    "        self.num_items = train_matrix.shape[1]\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.dropout = dropout\n",
    "        self.seed = seed\n",
    "        self.patience = 5\n",
    "        self.best_metric = 0\n",
    "        self.eval_topk = eval_topk\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.build_graph()\n",
    "\n",
    "    def build_graph(self):\n",
    "        ####### EDIT HERE #######\n",
    "        # Initialize W, W' and b, b' \n",
    "        self.W = nn.Parameter(torch.ones(self.num_items,self.hidden_dim))\n",
    "        self.W_prime = nn.Parameter(torch.ones(self.hidden_dim,self.num_items))\n",
    "        self.b = nn.Parameter(torch.ones(self.hidden_dim))\n",
    "        self.b_prime = nn.Parameter(torch.ones(self.num_items))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.normal_(self.b, 0, 0.001)\n",
    "        nn.init.xavier_uniform_(self.W_prime)\n",
    "        nn.init.normal_(self.b_prime, 0, 0.001)\n",
    "        #########################\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.reg_lambda)\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ####### EDIT HERE #######\n",
    "        # Denoise the input\n",
    "        x = F.dropout(x, p=self.dropout,training=self.training)\n",
    "    \n",
    "        # Encode the input\n",
    "        h = F.relu(torch.matmul(x,self.W) + self.b)\n",
    "\n",
    "\n",
    "        # Decode the latent representation\n",
    "   \n",
    "        output = torch.sigmoid(torch.matmul(h, self.W_prime) + self.b_prime)\n",
    "        #########################\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        train_matrix = torch.FloatTensor(self.train_matrix).to(self.device)\n",
    "        train_matrix = torch.FloatTensor(self.train_matrix).to(self.device)\n",
    "        for epoch in range(0, self.max_epochs):\n",
    "            self.train()\n",
    "            loss = self.train_model_per_batch(train_matrix)\n",
    "            if torch.isnan(loss):\n",
    "                print('Loss NAN. Train finish.')\n",
    "                break\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"[DAE] epoch %d, loss: %f\"%(epoch, loss))\n",
    "                prec, recall, ndcg = eval_implicit(self, self.valid_matrix_input, self.valid_matrix_target, self.eval_topk)\n",
    "                print(f\"(DAE VALID) prec@{self.eval_topk} {prec}, recall@{self.eval_topk} {recall}, ndcg@{self.eval_topk} {ndcg}\")\n",
    "                if self.check_early_stop(ndcg):\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "    def train_model_per_batch(self, train_matrix):\n",
    "\n",
    "        train_loader = DataLoader(train_matrix, batch_size=self.batch_size)\n",
    "        for batch_data in train_loader:\n",
    "            batch_data = batch_data.to(self.device).float()\n",
    "            # Initialize gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            output = self.forward(batch_data)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = F.binary_cross_entropy(output, batch_data, reduction='none').sum(1).mean()\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            self.optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def check_early_stop(self, metric):\n",
    "        ####### EDIT HERE #######\n",
    "        if metric > self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.best_epoch = 0\n",
    "            torch.save(self.state_dict(), f'checkpoints/{self.__class__.__name__}_best_model.pt')\n",
    "        else:\n",
    "            self.best_epoch += 1\n",
    "            if self.best_epoch > self.patience:\n",
    "                state_dict = torch.load(f'checkpoints/{self.__class__.__name__}_best_model.pt')\n",
    "                self.load_state_dict(state_dict)\n",
    "                return True\n",
    "\n",
    "\n",
    "\n",
    "        #########################\n",
    "        return False\n",
    "\n",
    "class MultVAE_implicit(torch.nn.Module):\n",
    "    def __init__(self, train_matrix, valid_matrix_input, valid_matrix_target, batch_size, max_epochs, hidden_dim, learning_rate, reg_lambda, dropout, eval_topk,seed, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.train_matrix = train_matrix\n",
    "        self.valid_matrix_input = valid_matrix_input\n",
    "        self.valid_matrix_target = valid_matrix_target\n",
    "\n",
    "        self.num_items = train_matrix.shape[1]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.total_anneal_steps = 200000\n",
    "        self.anneal_cap = 0.2\n",
    "        self.seed = seed\n",
    "        self.patience = 5\n",
    "        self.best_recall = 0\n",
    "        self.eval_topk = eval_topk\n",
    "\n",
    "        self.update_count = 0\n",
    "        self.device = device\n",
    "\n",
    "        self.build_graph()\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        ####### EDIT HERE #######\n",
    "        # Initialize W, W' and b, b'\n",
    "        self.W = nn.Parameter(torch.ones(self.num_items,self.hidden_dim))\n",
    "        self.W_prime = nn.Parameter(torch.ones(self.hidden_dim,self.num_items))\n",
    "        self.b = nn.Parameter(torch.ones(self.hidden_dim))\n",
    "        self.b_prime = nn.Parameter(torch.ones(self.num_items))\n",
    "   \n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.normal_(self.b, 0, 0.001)\n",
    "        nn.init.xavier_uniform_(self.W_prime)\n",
    "        nn.init.normal_(self.b, 0, 0.001)\n",
    "        \n",
    "\n",
    "        #########################\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.reg_lambda)\n",
    "\n",
    "        # 모델을 device로 보냄\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ####### EDIT HERE #######\n",
    "        # Denoise the input\n",
    "\n",
    "        x_denoised =F.dropout(x,p=self.dropout,training=self.training)\n",
    "       \n",
    "        # x_denoised = F.normalize(x)\n",
    "\n",
    "        # Encode the input\n",
    "        # h = F.relu(torch.matmul(x_denoised,self.W) + self.b)\n",
    "        h = torch.sigmoid(x_denoised@self.W + self.b)\n",
    "\n",
    "        z_log_var = torch.ones_like(h)\n",
    "        eps = torch.ones_like(h)\n",
    "        z = h + eps * torch.exp(z_log_var*0.5)\n",
    "    \n",
    "        # Decode the latent representationz\n",
    "        output = torch.matmul(z,self.W_prime) + self.b_prime\n",
    "        \n",
    "        print('h shape:', h.shape)\n",
    "        print('z_log_var shape:', z_log_var.shape)\n",
    "    \n",
    "        # KL loss\n",
    "       \n",
    "        kl_loss = -0.5 * torch.mean(torch.sum(1+z_log_var.unsqueeze(1)-h.pow(2)-z_log_var.exp().unsqueeze(1),dim=2))\n",
    "        #########################\n",
    "        \n",
    "\n",
    "        if self.training:\n",
    "            return output, kl_loss\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        train_matrix = torch.FloatTensor(self.train_matrix).to(self.device)\n",
    "        for epoch in range(0, self.max_epochs):\n",
    "            self.train()\n",
    "\n",
    "            loss = self.train_model_per_batch(self.train_matrix)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print('Loss NAN. Stop training')\n",
    "                break\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"[MultVAE CF] epoch %d, loss: %f\"%(epoch, loss))\n",
    "                prec, recall, ndcg = eval_implicit(self, self.valid_matrix_input, self.valid_matrix_target, self.eval_topk)\n",
    "                print(f\"(MultVAE VALID) prec@{self.eval_topk} {prec}, recall@{self.eval_topk} {recall}, ndcg@{self.eval_topk} {ndcg}\")\n",
    "                if self.check_early_stop(ndcg):\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "\n",
    "    def train_model_per_batch(self, train_matrix):\n",
    "       \n",
    "        train_loader = DataLoader(train_matrix, batch_size=self.batch_size)\n",
    "        for batch_data in train_loader:\n",
    "            batch_data = batch_data.to(self.device).float()\n",
    "            # Initialize gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            output, kl_loss = self.forward(batch_data)\n",
    "\n",
    "            # Calculate the loss\n",
    "            ce_loss = -(F.log_softmax(output, 1) * batch_data).sum(1).mean()\n",
    "\n",
    "            if self.total_anneal_steps > 0:\n",
    "                self.anneal = min(self.anneal_cap, 1. * self.update_count / self.total_anneal_steps)\n",
    "            else:\n",
    "                self.anneal = self.anneal_cap\n",
    "            loss = ce_loss + kl_loss * self.anneal\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.update_count += 1\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def check_early_stop(self, metric):\n",
    "        ####### EDIT HERE #######\n",
    "        if metric > self.best_recall:\n",
    "            self.best_recall = metric\n",
    "            self.num_patience = 0\n",
    "        else:\n",
    "            self.num_patience += 1\n",
    "        if self.num_patience >= self.patience:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "        #########################\n",
    "\n",
    "class AE_implicit(torch.nn.Module):\n",
    "    def __init__(self, train_matrix, valid_matrix_input, valid_matrix_target, batch_size, max_epochs, hidden_dim, learning_rate, reg_lambda, eval_topk,seed, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.train_matrix = train_matrix\n",
    "        self.valid_matrix_input = valid_matrix_input\n",
    "        self.valid_matrix_target = valid_matrix_target\n",
    "\n",
    "        self.num_items = train_matrix.shape[1]\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "\n",
    "        self.patience = 5\n",
    "        self.best_metric = 0\n",
    "        self.eval_topk = eval_topk\n",
    "\n",
    "        self.device = device\n",
    "        self.seed = seed\n",
    "        self.build_graph()\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        # W, W'와 b, b'만들기\n",
    "        self.enc_w = nn.Parameter(torch.ones(self.num_items, self.hidden_dim))\n",
    "        self.enc_b = nn.Parameter(torch.ones(self.hidden_dim))\n",
    "        nn.init.xavier_uniform_(self.enc_w)\n",
    "        nn.init.normal_(self.enc_b, 0, 0.001)\n",
    "\n",
    "        self.dec_w = nn.Parameter(torch.ones(self.hidden_dim, self.num_items))\n",
    "        self.dec_b = nn.Parameter(torch.ones(self.num_items))\n",
    "        nn.init.xavier_uniform_(self.dec_w)\n",
    "        nn.init.normal_(self.dec_b, 0, 0.001)\n",
    "\n",
    "        # 최적화 방법 설정\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=self.reg_lambda)\n",
    "\n",
    "        # 모델을 device로 보냄\n",
    "        self.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder 과정\n",
    "        h = torch.sigmoid(x @ self.enc_w + self.enc_b)\n",
    "\n",
    "        # decoder 과정\n",
    "        output = torch.sigmoid(h @ self.dec_w + self.dec_b)\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        train_matrix = torch.FloatTensor(self.train_matrix).to(self.device)\n",
    "\n",
    "        for epoch in range(0, self.max_epochs):\n",
    "            self.train()\n",
    "            loss = self.train_model_per_batch(train_matrix)\n",
    "            if torch.isnan(loss):\n",
    "                print('Loss NAN. Train finish.')\n",
    "                break\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"[AE] epoch %d, loss: %f\"%(epoch, loss))\n",
    "                prec, recall, ndcg = eval_implicit(self, self.valid_matrix_input, self.valid_matrix_target, self.eval_topk)\n",
    "                print(f\"(AE VALID) prec@{self.eval_topk} {prec:.5f}, recall@{self.eval_topk} {recall:.5f}, ndcg@{self.eval_topk} {ndcg:.5f}\")\n",
    "                if self.check_early_stop(ndcg):\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break            \n",
    "\n",
    "    def train_model_per_batch(self, train_matrix):\n",
    "        \n",
    "        train_loader = DataLoader(train_matrix, batch_size=self.batch_size)\n",
    "        for batch_data in train_loader:\n",
    "            batch_data = batch_data.to(self.device).float()\n",
    "            # Initialize gradients\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Forward\n",
    "            output = self.forward(batch_data)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = F.binary_cross_entropy(output, batch_data, reduction='none').sum(1).mean()\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def check_early_stop(self, metric):\n",
    "        if metric > self.best_metric:\n",
    "            self.best_metric = metric\n",
    "            self.best_epoch = 0\n",
    "            torch.save(self.state_dict(), f'checkpoints/{self.__class__.__name__}_best_model.pt')\n",
    "        else:\n",
    "            self.best_epoch += 1\n",
    "            if self.best_epoch > self.patience:\n",
    "                state_dict = torch.load(f'checkpoints/{self.__class__.__name__}_best_model.pt')\n",
    "                self.load_state_dict(state_dict)\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "# of users: 17017\n",
      "# of items: 7813\n",
      "# of train users (ratings): 13613 (984359)\n",
      "# of valid users (# of input ratings, # of target ratings): 1703 (101464, 25367)\n",
      "# of test users (# of input ratings, # of target ratings): 1701 (78141, 19536)\n",
      "############################################\n",
      "Training the model...\n"
     ]
    }
   ],
   "source": [
    "# 기본 패키지 import\n",
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# def seed_everything(random_seed):\n",
    "#     np.random.seed(random_seed)\n",
    "#     random.seed(random_seed)\n",
    "#     torch.manual_seed(random_seed)\n",
    "\n",
    "# seed = 506\n",
    "# seed_everything(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "dataset loading\n",
    "\"\"\"\n",
    "data_path = 'data'\n",
    "train_matrix, valid_matrix_input, valid_matrix_target, test_matrix_input, test_matrix_target,train_data, valid_data,test_data, train_valid, rating_cnt,num_items = load_data(data_path, implicit=True)\n",
    "topk = 20\n",
    "\n",
    "\"\"\"\n",
    "model training\n",
    "# \"\"\"\n",
    "print(\"Training the model...\")\n",
    "time_start = time()\n",
    "ae = AE_implicit(train_matrix, valid_matrix_input, valid_matrix_target, batch_size=2048, max_epochs=1000, hidden_dim=512, learning_rate=0.001, reg_lambda=0.001, eval_topk=topk,seed=506, device=device)\n",
    "dae = DAE_implicit(train_matrix, valid_matrix_input, valid_matrix_target, batch_size=2048, max_epochs=1000, hidden_dim=512, learning_rate=0.001, reg_lambda=0.001, dropout=0.2, eval_topk=topk,seed=506, device=device)\n",
    "multvae = MultVAE_implicit(train_matrix, valid_matrix_input, valid_matrix_target, batch_size=2048, max_epochs=1000, hidden_dim=512, learning_rate=0.001, reg_lambda=0.001, dropout=0.2, eval_topk=topk,seed=506, device=device) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define hyperparameters and their possible values\n",
    "hyperparams = {\n",
    "    'batch_size': [2048, 4096],\n",
    "    'max_epochs': [1000, 2000],\n",
    "    'hidden_dim': [128,256, 512],\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'reg_lambda': [0.001, 0.01],\n",
    "    # 'dropout': [0.3,0.5, 0.7],\n",
    "    'eval_topk': [20,50,100],\n",
    "    'seed': [42, 506, 123]\n",
    "}\n",
    "\n",
    "# Generate all possible combinations of hyperparameters\n",
    "param_grid = ParameterGrid(hyperparams)\n",
    "\n",
    "# Train and evaluate models for each combination of hyperparameters\n",
    "best_model = None\n",
    "best_metric = 0\n",
    "for params in param_grid:\n",
    "    print(f\"Training model with hyperparameters: {params}\")\n",
    "    model = AE_implicit(train_matrix, valid_matrix_input, valid_matrix_target, device=device, **params)\n",
    "    model.fit()\n",
    "    _, _, ndcg = eval_implicit(model, test_matrix_input, test_matrix_target, params['eval_topk'])\n",
    "    print(f\"Evaluated model with ndcg@{params['eval_topk']}: {ndcg}\")\n",
    "    if ndcg > best_metric:\n",
    "        best_metric = ndcg\n",
    "        best_model = model\n",
    "\n",
    "# Print best performing model and its hyperparameters\n",
    "print(f\"Best performing model with ndcg@{best_model.eval_topk}: {best_metric}\")\n",
    "print(f\"Hyperparameters: {best_model.__dict__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AE] epoch 0, loss: 1657.729614\n",
      "(AE VALID) prec@20 0.00232, recall@20 0.00347, ndcg@20 0.00244\n",
      "[AE] epoch 10, loss: 1778.862305\n",
      "(AE VALID) prec@20 0.02023, recall@20 0.03424, ndcg@20 0.02825\n",
      "[AE] epoch 20, loss: 1631.737549\n",
      "(AE VALID) prec@20 0.02043, recall@20 0.03434, ndcg@20 0.02858\n",
      "[AE] epoch 30, loss: 1435.873169\n",
      "(AE VALID) prec@20 0.02240, recall@20 0.03799, ndcg@20 0.03069\n",
      "[AE] epoch 40, loss: 1290.915771\n",
      "(AE VALID) prec@20 0.02408, recall@20 0.04085, ndcg@20 0.03268\n",
      "[AE] epoch 50, loss: 1204.558716\n",
      "(AE VALID) prec@20 0.02504, recall@20 0.04213, ndcg@20 0.03552\n",
      "[AE] epoch 60, loss: 1150.796265\n",
      "(AE VALID) prec@20 0.02686, recall@20 0.04450, ndcg@20 0.03768\n",
      "[AE] epoch 70, loss: 1112.673340\n",
      "(AE VALID) prec@20 0.02954, recall@20 0.04962, ndcg@20 0.04242\n",
      "[AE] epoch 80, loss: 1082.592773\n",
      "(AE VALID) prec@20 0.03371, recall@20 0.05864, ndcg@20 0.04976\n",
      "[AE] epoch 90, loss: 1058.761841\n",
      "(AE VALID) prec@20 0.03644, recall@20 0.06442, ndcg@20 0.05438\n",
      "[AE] epoch 100, loss: 1038.859131\n",
      "(AE VALID) prec@20 0.03858, recall@20 0.06903, ndcg@20 0.05783\n",
      "[AE] epoch 110, loss: 1021.760010\n",
      "(AE VALID) prec@20 0.04005, recall@20 0.07178, ndcg@20 0.06038\n",
      "[AE] epoch 120, loss: 1006.487183\n",
      "(AE VALID) prec@20 0.04228, recall@20 0.07549, ndcg@20 0.06363\n",
      "[AE] epoch 130, loss: 992.467651\n",
      "(AE VALID) prec@20 0.04416, recall@20 0.07918, ndcg@20 0.06700\n",
      "[AE] epoch 140, loss: 978.984924\n",
      "(AE VALID) prec@20 0.04677, recall@20 0.08358, ndcg@20 0.07100\n",
      "[AE] epoch 150, loss: 965.670471\n",
      "(AE VALID) prec@20 0.04988, recall@20 0.08873, ndcg@20 0.07522\n",
      "[AE] epoch 160, loss: 952.410828\n",
      "(AE VALID) prec@20 0.05314, recall@20 0.09528, ndcg@20 0.08003\n",
      "[AE] epoch 170, loss: 939.295288\n",
      "(AE VALID) prec@20 0.05564, recall@20 0.09954, ndcg@20 0.08450\n",
      "[AE] epoch 180, loss: 927.604065\n",
      "(AE VALID) prec@20 0.05737, recall@20 0.10387, ndcg@20 0.08823\n",
      "[AE] epoch 190, loss: 915.304932\n",
      "(AE VALID) prec@20 0.05887, recall@20 0.10736, ndcg@20 0.09191\n",
      "[AE] epoch 200, loss: 903.559814\n",
      "(AE VALID) prec@20 0.06069, recall@20 0.11109, ndcg@20 0.09529\n",
      "[AE] epoch 210, loss: 892.780090\n",
      "(AE VALID) prec@20 0.06174, recall@20 0.11347, ndcg@20 0.09733\n",
      "[AE] epoch 220, loss: 882.590942\n",
      "(AE VALID) prec@20 0.06327, recall@20 0.11638, ndcg@20 0.09980\n",
      "[AE] epoch 230, loss: 874.261536\n",
      "(AE VALID) prec@20 0.06436, recall@20 0.11846, ndcg@20 0.10221\n",
      "[AE] epoch 240, loss: 863.201233\n",
      "(AE VALID) prec@20 0.06509, recall@20 0.12052, ndcg@20 0.10456\n",
      "[AE] epoch 250, loss: 853.749634\n",
      "(AE VALID) prec@20 0.06588, recall@20 0.12253, ndcg@20 0.10660\n",
      "[AE] epoch 260, loss: 844.555542\n",
      "(AE VALID) prec@20 0.06709, recall@20 0.12512, ndcg@20 0.10870\n",
      "[AE] epoch 270, loss: 843.384705\n",
      "(AE VALID) prec@20 0.06820, recall@20 0.12714, ndcg@20 0.11047\n",
      "[AE] epoch 280, loss: 827.467712\n",
      "(AE VALID) prec@20 0.06932, recall@20 0.12885, ndcg@20 0.11207\n",
      "[AE] epoch 290, loss: 818.642822\n",
      "(AE VALID) prec@20 0.07041, recall@20 0.13151, ndcg@20 0.11385\n",
      "[AE] epoch 300, loss: 810.244690\n",
      "(AE VALID) prec@20 0.07123, recall@20 0.13280, ndcg@20 0.11497\n",
      "[AE] epoch 310, loss: 808.212952\n",
      "(AE VALID) prec@20 0.07190, recall@20 0.13377, ndcg@20 0.11639\n",
      "[AE] epoch 320, loss: 793.089355\n",
      "(AE VALID) prec@20 0.07184, recall@20 0.13351, ndcg@20 0.11730\n",
      "[AE] epoch 330, loss: 783.983704\n",
      "(AE VALID) prec@20 0.07267, recall@20 0.13525, ndcg@20 0.11865\n",
      "[AE] epoch 340, loss: 777.500305\n",
      "(AE VALID) prec@20 0.07284, recall@20 0.13549, ndcg@20 0.11934\n",
      "[AE] epoch 350, loss: 767.002075\n",
      "(AE VALID) prec@20 0.07422, recall@20 0.13783, ndcg@20 0.12173\n",
      "[AE] epoch 360, loss: 755.400452\n",
      "(AE VALID) prec@20 0.07478, recall@20 0.13888, ndcg@20 0.12274\n",
      "[AE] epoch 370, loss: 746.624756\n",
      "(AE VALID) prec@20 0.07522, recall@20 0.13981, ndcg@20 0.12424\n",
      "[AE] epoch 380, loss: 737.003235\n",
      "(AE VALID) prec@20 0.07581, recall@20 0.14104, ndcg@20 0.12557\n",
      "[AE] epoch 390, loss: 722.909729\n",
      "(AE VALID) prec@20 0.07628, recall@20 0.14148, ndcg@20 0.12593\n",
      "[AE] epoch 400, loss: 712.113892\n",
      "(AE VALID) prec@20 0.07701, recall@20 0.14353, ndcg@20 0.12699\n",
      "[AE] epoch 410, loss: 701.004456\n",
      "(AE VALID) prec@20 0.07745, recall@20 0.14443, ndcg@20 0.12821\n",
      "[AE] epoch 420, loss: 686.497009\n",
      "(AE VALID) prec@20 0.07830, recall@20 0.14565, ndcg@20 0.12934\n",
      "[AE] epoch 430, loss: 673.745239\n",
      "(AE VALID) prec@20 0.07868, recall@20 0.14636, ndcg@20 0.13040\n",
      "[AE] epoch 440, loss: 660.925415\n",
      "(AE VALID) prec@20 0.07898, recall@20 0.14701, ndcg@20 0.13094\n",
      "[AE] epoch 450, loss: 646.723022\n",
      "(AE VALID) prec@20 0.07965, recall@20 0.14814, ndcg@20 0.13220\n",
      "[AE] epoch 460, loss: 632.368408\n",
      "(AE VALID) prec@20 0.07977, recall@20 0.14870, ndcg@20 0.13332\n",
      "[AE] epoch 470, loss: 617.960205\n",
      "(AE VALID) prec@20 0.07998, recall@20 0.14954, ndcg@20 0.13394\n",
      "[AE] epoch 480, loss: 603.159790\n",
      "(AE VALID) prec@20 0.07971, recall@20 0.14917, ndcg@20 0.13395\n",
      "[AE] epoch 490, loss: 588.080444\n",
      "(AE VALID) prec@20 0.08015, recall@20 0.14979, ndcg@20 0.13442\n",
      "[AE] epoch 500, loss: 572.075806\n",
      "(AE VALID) prec@20 0.08030, recall@20 0.15039, ndcg@20 0.13412\n",
      "[AE] epoch 510, loss: 555.151184\n",
      "(AE VALID) prec@20 0.07977, recall@20 0.14925, ndcg@20 0.13391\n",
      "[AE] epoch 520, loss: 537.843750\n",
      "(AE VALID) prec@20 0.07989, recall@20 0.15012, ndcg@20 0.13379\n",
      "[AE] epoch 530, loss: 520.000916\n",
      "(AE VALID) prec@20 0.07951, recall@20 0.14902, ndcg@20 0.13307\n",
      "[AE] epoch 540, loss: 501.961456\n",
      "(AE VALID) prec@20 0.07898, recall@20 0.14844, ndcg@20 0.13216\n",
      "[AE] epoch 550, loss: 483.554657\n",
      "(AE VALID) prec@20 0.07821, recall@20 0.14684, ndcg@20 0.13107\n",
      "Early stopping at epoch 550\n",
      "[DAE] epoch 0, loss: 2580.537354\n",
      "(DAE VALID) prec@20 0.00995302407516148, recall@20 0.016529847518573476, ndcg@20 0.013187915798419663\n",
      "[DAE] epoch 10, loss: 7159.664062\n",
      "(DAE VALID) prec@20 0.02108044627128597, recall@20 0.033975079917929814, ndcg@20 0.028459796108101416\n",
      "[DAE] epoch 20, loss: 2405.934326\n",
      "(DAE VALID) prec@20 0.02410452143276571, recall@20 0.03949229240692675, ndcg@20 0.0331484736442354\n",
      "[DAE] epoch 30, loss: 1476.022461\n",
      "(DAE VALID) prec@20 0.031914268937169706, recall@20 0.0530697839222088, ndcg@20 0.0443902897919604\n",
      "[DAE] epoch 40, loss: 1200.109375\n",
      "(DAE VALID) prec@20 0.03769817968291251, recall@20 0.06397045713790522, ndcg@20 0.05471574482955738\n",
      "[DAE] epoch 50, loss: 1064.680176\n",
      "(DAE VALID) prec@20 0.04142689371697005, recall@20 0.07123623987776485, ndcg@20 0.061370365257734504\n",
      "[DAE] epoch 60, loss: 982.400269\n",
      "(DAE VALID) prec@20 0.046007046388725784, recall@20 0.07962657546464305, ndcg@20 0.06795176441450522\n",
      "[DAE] epoch 70, loss: 926.499268\n",
      "(DAE VALID) prec@20 0.04947152084556665, recall@20 0.08643861238626621, ndcg@20 0.07382004918012844\n",
      "[DAE] epoch 80, loss: 875.168762\n",
      "(DAE VALID) prec@20 0.05255431591309454, recall@20 0.09259607660960491, ndcg@20 0.07947484550418976\n",
      "[DAE] epoch 90, loss: 839.443970\n",
      "(DAE VALID) prec@20 0.05543159130945391, recall@20 0.09766564909212921, ndcg@20 0.08514236418324922\n",
      "[DAE] epoch 100, loss: 807.056763\n",
      "(DAE VALID) prec@20 0.05836758661186142, recall@20 0.10381193708712619, ndcg@20 0.09033814412511428\n",
      "[DAE] epoch 110, loss: 776.019592\n",
      "(DAE VALID) prec@20 0.06109806224310041, recall@20 0.10897765659595539, ndcg@20 0.09489843115476668\n",
      "[DAE] epoch 120, loss: 746.338867\n",
      "(DAE VALID) prec@20 0.06250733998825601, recall@20 0.11242611601839261, ndcg@20 0.09858659484648037\n",
      "[DAE] epoch 130, loss: 715.584717\n",
      "(DAE VALID) prec@20 0.06473869641808573, recall@20 0.11708533399632606, ndcg@20 0.10216273904387402\n",
      "[DAE] epoch 140, loss: 684.243591\n",
      "(DAE VALID) prec@20 0.06661773341162654, recall@20 0.1213793735395798, ndcg@20 0.10510766738963002\n",
      "[DAE] epoch 150, loss: 652.278320\n",
      "(DAE VALID) prec@20 0.06790957134468587, recall@20 0.12393917370395321, ndcg@20 0.10757139120856579\n",
      "[DAE] epoch 160, loss: 619.752441\n",
      "(DAE VALID) prec@20 0.06893716970052849, recall@20 0.1263526523552993, ndcg@20 0.10965949723099483\n",
      "[DAE] epoch 170, loss: 589.632874\n",
      "(DAE VALID) prec@20 0.07055196711685262, recall@20 0.12925331445330757, ndcg@20 0.1123566894339344\n",
      "[DAE] epoch 180, loss: 557.338989\n",
      "(DAE VALID) prec@20 0.07178508514386378, recall@20 0.1313385432781136, ndcg@20 0.11423730504565806\n",
      "[DAE] epoch 190, loss: 520.338074\n",
      "(DAE VALID) prec@20 0.07310628302994715, recall@20 0.1347019001745954, ndcg@20 0.11685529655643401\n",
      "[DAE] epoch 200, loss: 527.180298\n",
      "(DAE VALID) prec@20 0.07422196124486201, recall@20 0.13707595928417313, ndcg@20 0.11913149102292728\n",
      "[DAE] epoch 210, loss: 457.168488\n",
      "(DAE VALID) prec@20 0.07530827950675278, recall@20 0.1398439597085585, ndcg@20 0.12115145076149089\n",
      "[DAE] epoch 220, loss: 427.204437\n",
      "(DAE VALID) prec@20 0.07580739870816207, recall@20 0.14104659024360006, ndcg@20 0.12308587186663138\n",
      "[DAE] epoch 230, loss: 400.562866\n",
      "(DAE VALID) prec@20 0.07630651790957134, recall@20 0.14222470415184418, ndcg@20 0.12435598378421699\n",
      "[DAE] epoch 240, loss: 370.102753\n",
      "(DAE VALID) prec@20 0.07721667645331767, recall@20 0.1435710320425011, ndcg@20 0.1257129211172311\n",
      "[DAE] epoch 250, loss: 351.866547\n",
      "(DAE VALID) prec@20 0.07762771579565474, recall@20 0.14426483738433693, ndcg@20 0.12694780704483846\n",
      "[DAE] epoch 260, loss: 319.535706\n",
      "(DAE VALID) prec@20 0.07853787433940106, recall@20 0.14652796533560583, ndcg@20 0.12862850458182118\n",
      "[DAE] epoch 270, loss: 353.862976\n",
      "(DAE VALID) prec@20 0.07853787433940106, recall@20 0.14663859287102007, ndcg@20 0.12975422181495327\n",
      "[DAE] epoch 280, loss: 274.703156\n",
      "(DAE VALID) prec@20 0.07953611274221963, recall@20 0.1489534943198029, ndcg@20 0.13140789395337435\n",
      "[DAE] epoch 290, loss: 256.793640\n",
      "(DAE VALID) prec@20 0.07997651203758073, recall@20 0.14954246181905803, ndcg@20 0.1329046961080855\n",
      "[DAE] epoch 300, loss: 239.811249\n",
      "(DAE VALID) prec@20 0.07947739283617147, recall@20 0.14997262741104878, ndcg@20 0.13296856147055067\n",
      "[DAE] epoch 310, loss: 218.422501\n",
      "(DAE VALID) prec@20 0.08006459189665296, recall@20 0.15079508796892435, ndcg@20 0.13426457035545486\n",
      "[DAE] epoch 320, loss: 202.411362\n",
      "(DAE VALID) prec@20 0.08000587199060481, recall@20 0.15070095727913202, ndcg@20 0.13484390530858578\n",
      "[DAE] epoch 330, loss: 199.231552\n",
      "(DAE VALID) prec@20 0.07886083382266589, recall@20 0.1490998705994688, ndcg@20 0.1325533281088827\n",
      "[DAE] epoch 340, loss: 175.563568\n",
      "(DAE VALID) prec@20 0.08009395184967705, recall@20 0.15103815952395858, ndcg@20 0.134681205392879\n",
      "[DAE] epoch 350, loss: 161.935852\n",
      "(DAE VALID) prec@20 0.07953611274221961, recall@20 0.1494827442531276, ndcg@20 0.1339700229122313\n",
      "[DAE] epoch 360, loss: 149.841751\n",
      "(DAE VALID) prec@20 0.07974163241338815, recall@20 0.1499655561077401, ndcg@20 0.13397621108101326\n",
      "[DAE] epoch 370, loss: 146.880920\n",
      "(DAE VALID) prec@20 0.07897827363476219, recall@20 0.14928833756835336, ndcg@20 0.1328774754902789\n",
      "[DAE] epoch 380, loss: 130.219757\n",
      "(DAE VALID) prec@20 0.07950675278919553, recall@20 0.14968407836779804, ndcg@20 0.1330933635727351\n",
      "Early stopping at epoch 380\n",
      "[MultVAE CF] epoch 0, loss: 2760.938721\n",
      "(MultVAE VALID) prec@20 0.018144450968878453, recall@20 0.030791710653191667, ndcg@20 0.026424170734945587\n",
      "[MultVAE CF] epoch 10, loss: 2681.072021\n",
      "(MultVAE VALID) prec@20 0.02507339988256019, recall@20 0.041993158184445094, ndcg@20 0.033966819506724776\n",
      "[MultVAE CF] epoch 20, loss: 2662.105469\n",
      "(MultVAE VALID) prec@20 0.03150322959483264, recall@20 0.0521372966879576, ndcg@20 0.04355611507313719\n",
      "[MultVAE CF] epoch 30, loss: 2625.606934\n",
      "(MultVAE VALID) prec@20 0.040751614797416326, recall@20 0.06935456323414925, ndcg@20 0.0616239378584628\n",
      "[MultVAE CF] epoch 40, loss: 2590.190430\n",
      "(MultVAE VALID) prec@20 0.044715208455666476, recall@20 0.0765572576995553, ndcg@20 0.06856371737722616\n",
      "[MultVAE CF] epoch 50, loss: 2564.037354\n",
      "(MultVAE VALID) prec@20 0.04873752201996478, recall@20 0.08452885929443416, ndcg@20 0.0757617567653823\n",
      "[MultVAE CF] epoch 60, loss: 2545.090576\n",
      "(MultVAE VALID) prec@20 0.05220199647680564, recall@20 0.09117347533337164, ndcg@20 0.08181921271387829\n",
      "[MultVAE CF] epoch 70, loss: 2529.953857\n",
      "(MultVAE VALID) prec@20 0.054726952436876114, recall@20 0.09711573112182853, ndcg@20 0.08643700346900397\n",
      "[MultVAE CF] epoch 80, loss: 2517.313232\n",
      "(MultVAE VALID) prec@20 0.05657662947739283, recall@20 0.10136339617136572, ndcg@20 0.09004994192544126\n",
      "[MultVAE CF] epoch 90, loss: 2506.003906\n",
      "(MultVAE VALID) prec@20 0.057339988256018784, recall@20 0.10301336909080665, ndcg@20 0.09188844553410655\n",
      "[MultVAE CF] epoch 100, loss: 2497.033447\n",
      "(MultVAE VALID) prec@20 0.05924838520258368, recall@20 0.10701581574669769, ndcg@20 0.09494507491888832\n",
      "[MultVAE CF] epoch 110, loss: 2488.179443\n",
      "(MultVAE VALID) prec@20 0.059835584263065185, recall@20 0.10859098586267814, ndcg@20 0.09664826545849958\n",
      "[MultVAE CF] epoch 120, loss: 2479.873535\n",
      "(MultVAE VALID) prec@20 0.060980622431004106, recall@20 0.11108378103491444, ndcg@20 0.09937131948230837\n",
      "[MultVAE CF] epoch 130, loss: 2472.122314\n",
      "(MultVAE VALID) prec@20 0.06168526130358192, recall@20 0.11313473295374657, ndcg@20 0.1010618194477484\n",
      "[MultVAE CF] epoch 140, loss: 2465.648926\n",
      "(MultVAE VALID) prec@20 0.06221374045801527, recall@20 0.1144077495893586, ndcg@20 0.10146111137823277\n",
      "[MultVAE CF] epoch 150, loss: 2458.973877\n",
      "(MultVAE VALID) prec@20 0.062448620082207876, recall@20 0.11529921212645108, ndcg@20 0.10252481268844817\n",
      "[MultVAE CF] epoch 160, loss: 2451.657471\n",
      "(MultVAE VALID) prec@20 0.06365237815619496, recall@20 0.11748746110746605, ndcg@20 0.10424416029746128\n",
      "[MultVAE CF] epoch 170, loss: 2445.199463\n",
      "(MultVAE VALID) prec@20 0.06412213740458016, recall@20 0.11809019321204156, ndcg@20 0.10489924018273422\n",
      "[MultVAE CF] epoch 180, loss: 2439.813965\n",
      "(MultVAE VALID) prec@20 0.06509101585437464, recall@20 0.12036665503628284, ndcg@20 0.1062118819896475\n",
      "[MultVAE CF] epoch 190, loss: 2433.208008\n",
      "(MultVAE VALID) prec@20 0.06506165590135056, recall@20 0.12088783249521722, ndcg@20 0.10733749217539149\n",
      "[MultVAE CF] epoch 200, loss: 2428.619873\n",
      "(MultVAE VALID) prec@20 0.06600117439812098, recall@20 0.12310799865695252, ndcg@20 0.1088464027636994\n",
      "[MultVAE CF] epoch 210, loss: 2421.274658\n",
      "(MultVAE VALID) prec@20 0.06623605402231357, recall@20 0.12309932259174991, ndcg@20 0.10876682006721017\n",
      "[MultVAE CF] epoch 220, loss: 2416.501465\n",
      "(MultVAE VALID) prec@20 0.06641221374045803, recall@20 0.12392894494955782, ndcg@20 0.1092895807606844\n",
      "[MultVAE CF] epoch 230, loss: 2411.803955\n",
      "(MultVAE VALID) prec@20 0.06738109219025251, recall@20 0.1263957301026969, ndcg@20 0.11064491220458754\n",
      "[MultVAE CF] epoch 240, loss: 2405.717285\n",
      "(MultVAE VALID) prec@20 0.0682031708749266, recall@20 0.1277356686623297, ndcg@20 0.11236274475592337\n",
      "[MultVAE CF] epoch 250, loss: 2401.267334\n",
      "(MultVAE VALID) prec@20 0.06802701115678216, recall@20 0.12741602673781635, ndcg@20 0.11260589816257807\n",
      "[MultVAE CF] epoch 260, loss: 2394.939209\n",
      "(MultVAE VALID) prec@20 0.06773341162654141, recall@20 0.1273984641296742, ndcg@20 0.1122052603135445\n",
      "[MultVAE CF] epoch 270, loss: 2390.111084\n",
      "(MultVAE VALID) prec@20 0.06805637110980622, recall@20 0.12777368084879506, ndcg@20 0.11288190243840862\n",
      "[MultVAE CF] epoch 280, loss: 2385.837646\n",
      "(MultVAE VALID) prec@20 0.06879036993540812, recall@20 0.12857422195502966, ndcg@20 0.11358799285967411\n",
      "[MultVAE CF] epoch 290, loss: 2380.356689\n",
      "(MultVAE VALID) prec@20 0.0697298884321785, recall@20 0.1302204140871199, ndcg@20 0.11441006345451583\n",
      "[MultVAE CF] epoch 300, loss: 2375.220459\n",
      "(MultVAE VALID) prec@20 0.06937756899588961, recall@20 0.1293054837099084, ndcg@20 0.11379077929814702\n",
      "[MultVAE CF] epoch 310, loss: 2371.866943\n",
      "(MultVAE VALID) prec@20 0.06926012918379332, recall@20 0.12971638759108148, ndcg@20 0.11341441577942966\n",
      "[MultVAE CF] epoch 320, loss: 2367.298096\n",
      "(MultVAE VALID) prec@20 0.06940692894891369, recall@20 0.13051782363948203, ndcg@20 0.11452638908878103\n",
      "[MultVAE CF] epoch 330, loss: 2363.411621\n",
      "(MultVAE VALID) prec@20 0.0701115678214915, recall@20 0.13165149202169102, ndcg@20 0.11513553271596483\n",
      "[MultVAE CF] epoch 340, loss: 2358.449463\n",
      "(MultVAE VALID) prec@20 0.06984732824427482, recall@20 0.13114997501667627, ndcg@20 0.11492085048295281\n",
      "[MultVAE CF] epoch 350, loss: 2353.724609\n",
      "(MultVAE VALID) prec@20 0.07019964768056372, recall@20 0.13184578063986374, ndcg@20 0.11500284877568366\n",
      "[MultVAE CF] epoch 360, loss: 2348.792969\n",
      "(MultVAE VALID) prec@20 0.07025836758661186, recall@20 0.13209172089271679, ndcg@20 0.11466891268725539\n",
      "[MultVAE CF] epoch 370, loss: 2345.023438\n",
      "(MultVAE VALID) prec@20 0.06975924838520259, recall@20 0.1311458843659804, ndcg@20 0.11423173180377887\n",
      "[MultVAE CF] epoch 380, loss: 2342.590576\n",
      "(MultVAE VALID) prec@20 0.06999412800939518, recall@20 0.13076035116292792, ndcg@20 0.11426139835369728\n",
      "Early stopping at epoch 380\n",
      "training time:  535.1977255344391\n",
      "model evaluation\n",
      "evaluation time:  3.6287169456481934\n",
      "AE: prec@20 0.059024, recall@20 0.14097, ndcg@20 0.11252\n",
      "DAE: prec@20 0.05873, recall@20 0.14064, ndcg@20 0.11193\n",
      "MultVAE: prec@20 0.051646, recall@20 0.1205, ndcg@20 0.096963\n"
     ]
    }
   ],
   "source": [
    "ae.fit()\n",
    "dae.fit()\n",
    "multvae.fit()\n",
    "print(\"training time: \", time()-time_start)\n",
    "time_start = time()\n",
    "\"\"\"\n",
    "model evaluation\n",
    "\"\"\"\n",
    "print(\"model evaluation\")\n",
    "ae_prec, ae_recall, ae_ndcg = eval_implicit(ae, test_matrix_input, test_matrix_target, topk)\n",
    "dae_prec, dae_recall, dae_ndcg = eval_implicit(dae, test_matrix_input, test_matrix_target, topk)\n",
    "multvae_prec, multvae_recall, multvae_ndcg = eval_implicit(multvae, test_matrix_input, test_matrix_target, topk)\n",
    "print(\"evaluation time: \", time()-time_start)\n",
    "\n",
    "print(f\"AE: prec@{topk} {ae_prec:.5}, recall@{topk} {ae_recall:.5f}, ndcg@{topk} {ae_ndcg:.5}\")\n",
    "print(f\"DAE: prec@{topk} {dae_prec:.5}, recall@{topk} {dae_recall:.5}, ndcg@{topk} {dae_ndcg:.5}\")\n",
    "print(f\"MultVAE: prec@{topk} {multvae_prec:.5}, recall@{topk} {multvae_recall:.5}, ndcg@{topk} {multvae_ndcg:.5}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1701/1701 [00:02<00:00, 719.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# autoencoder\n",
    "num_users_test = test_data['user_id'].unique().shape[0]\n",
    "test_indices = np.arange(test_matrix_input.shape[0])\n",
    "test_loader = DataLoader(test_matrix_target, batch_size=ae.batch_size)\n",
    "with torch.no_grad():\n",
    "    ae.eval()\n",
    "\n",
    "    submission = pd.DataFrame(columns=['user_id', 'item_id'])\n",
    "\n",
    "    for user_id in tqdm(range(num_users_test)):\n",
    "        input_by_user = torch.FloatTensor(test_matrix_input[user_id]).to(ae.device)\n",
    "        preds = ae.forward(input_by_user).cpu().numpy()\n",
    "        missing_item_ids = np.where(test_matrix_input[user_id] == 0)[0]\n",
    "        pred_u_score = preds[missing_item_ids]\n",
    "        pred_u_idx = np.argsort(pred_u_score)[::-1]\n",
    "        pred_u = missing_item_ids[pred_u_idx]\n",
    "\n",
    "        submission = submission.append(pd.DataFrame({'user_id': [user_id] * 20, 'item_id': pred_u[:20]}))\n",
    "\n",
    "    submission.to_csv('submission_AE_model2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1701/1701 [00:02<00:00, 722.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# DAE\n",
    "num_users_test = test_data['user_id'].unique().shape[0]\n",
    "test_indices = np.arange(test_matrix_input.shape[0])\n",
    "test_loader = DataLoader(test_matrix_target, batch_size=dae.batch_size)\n",
    "with torch.no_grad():\n",
    "    dae.eval()\n",
    "\n",
    "    submission = pd.DataFrame(columns=['user_id', 'item_id'])\n",
    "\n",
    "    for user_id in tqdm(range(num_users_test)):\n",
    "        input_by_user = torch.FloatTensor(test_matrix_input[user_id]).to(dae.device)\n",
    "        preds = dae.forward(input_by_user).cpu().numpy()\n",
    "        missing_item_ids = np.where(test_matrix_input[user_id] == 0)[0]\n",
    "        pred_u_score = preds[missing_item_ids]\n",
    "        pred_u_idx = np.argsort(pred_u_score)[::-1]\n",
    "        pred_u = missing_item_ids[pred_u_idx]\n",
    "\n",
    "        submission = submission.append(pd.DataFrame({'user_id': [user_id] * 20, 'item_id': pred_u[:20]}))\n",
    "\n",
    "    submission.to_csv('submission_DAE_mode2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1701/1701 [00:02<00:00, 638.96it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_users_test = test_data['user_id'].unique().shape[0]\n",
    "test_indices = np.arange(test_matrix_input.shape[0])\n",
    "test_loader = DataLoader(test_matrix_target, batch_size=ae.batch_size)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    ae.eval()\n",
    "    dae.eval()\n",
    "    # multvae.eval()\n",
    "    submission = pd.DataFrame(columns=['user_id', 'item_id'])\n",
    "    for user_id in tqdm(range(num_users_test)):\n",
    "        input_by_user = torch.FloatTensor(test_matrix_input[user_id]).to(device)\n",
    "\n",
    "        # Get predictions from each model\n",
    "        ae_preds = ae.forward(input_by_user).cpu().numpy()\n",
    "        dae_preds = dae.forward(input_by_user).cpu().numpy()\n",
    "        # multvae_preds = multvae.forward(input_by_user).cpu().numpy()\n",
    "\n",
    "        # Ensemble the predictions\n",
    "        preds = (ae_preds + dae_preds) / 2\n",
    "\n",
    "        missing_item_ids = np.where(test_matrix_input[user_id] == 0)[0]\n",
    "        pred_u_score = preds[missing_item_ids]\n",
    "        pred_u_idx = np.argsort(pred_u_score)[::-1]\n",
    "        pred_u = missing_item_ids[pred_u_idx]\n",
    "\n",
    "        submission = submission.append(pd.DataFrame({'user_id': [user_id] * 20, 'item_id': pred_u[:20]}))\n",
    "\n",
    "submission.to_csv('submission_ensemble2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8addaabea1cb039ebd0cad50c57d5e545a2dd9c4864cd769240013056fee56e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
