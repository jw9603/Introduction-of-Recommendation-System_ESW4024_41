{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGCN\n",
    "- https://arxiv.org/abs/2002.02126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# Load and preprocess data\n",
    "data_dir = 'data/'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_data = pd.read_csv(os.path.join(data_dir, \"train_data.csv\"), names=None)\n",
    "train, valid = train_test_split(all_data, test_size=0.2, stratify=all_data['user_id'], random_state=1234)\n",
    "\n",
    "test_data = pd.read_csv(os.path.join(data_dir, \"test_data.csv\"), names=None)\n",
    "\n",
    "user_list = list(all_data['user_id'].unique())\n",
    "item_list = list(all_data['item_id'].unique())\n",
    "\n",
    "num_users = len(user_list)\n",
    "num_items = len(item_list)\n",
    "num_ratings = len(all_data)\n",
    "\n",
    "user_id_dict = {old_uid: new_uid for new_uid, old_uid in enumerate(user_list)}\n",
    "all_data.user_id = [user_id_dict[x] for x in all_data.user_id.tolist()]\n",
    "\n",
    "item_id_dict = {old_uid: new_uid for new_uid, old_uid in enumerate(item_list)}\n",
    "all_data.item_id = [item_id_dict[x] for x in all_data.item_id.tolist()]\n",
    "\n",
    "train = train[['user_id', 'item_id']].to_numpy()\n",
    "valid = valid[['user_id', 'item_id']].to_numpy()\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "\n",
    "# Encode user and item IDs\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "train[:, 0] = user_encoder.fit_transform(train[:, 0])\n",
    "train[:, 1] = item_encoder.fit_transform(train[:, 1])\n",
    "\n",
    "valid[:, 0] = user_encoder.transform(valid[:, 0])\n",
    "valid[:, 1] = item_encoder.transform(valid[:, 1])\n",
    "\n",
    "# Convert train and valid data to TensorDataset\n",
    "train_dataset = TensorDataset(torch.tensor(train, dtype=torch.long))\n",
    "valid_dataset = TensorDataset(torch.tensor(valid, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=80, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=40, shuffle=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# class GCNRec(nn.Module):\n",
    "#     def __init__(self, num_users, num_items, embedding_size, n_layers, dropout=0.5):\n",
    "#         super(GCNRec, self).__init__()\n",
    "#         self.embedding_size = embedding_size\n",
    "#         self.n_layers = n_layers\n",
    "#         self.user_embeddings = nn.Embedding(num_users, embedding_size)\n",
    "#         self.item_embeddings = nn.Embedding(num_items, embedding_size)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.layers = nn.ModuleList([\n",
    "#             GCNConv(embedding_size, embedding_size) for _ in range(n_layers)\n",
    "#         ])\n",
    "#         self.output_layer = nn.Linear(embedding_size, 1)\n",
    "\n",
    "#     def forward(self, users, items, edge_index):\n",
    "#         user_embeddings = self.user_embeddings(users)\n",
    "#         item_embeddings = self.item_embeddings(items)\n",
    "#         x = torch.cat([user_embeddings, item_embeddings], dim=0)\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, edge_index)\n",
    "#             x = F.relu(x)\n",
    "#             x = self.dropout(x)\n",
    "#         user_item_embeddings = self.output_layer(x[:num_users] * x[num_users:])\n",
    "#         return user_item_embeddings.squeeze()\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size, n_layers,dropout=0.5):\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "        self.user_embeddings = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embeddings = nn.Embedding(num_items, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.layers = nn.ModuleList([GraphConv(embedding_size, embedding_size) for _ in range(n_layers)])\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(embedding_size, embedding_size) for _ in range(n_layers)\n",
    "        ])\n",
    "    def forward(self, users, items, edge_index):\n",
    "        user_embeddings = self.user_embeddings(users)\n",
    "        item_embeddings = self.item_embeddings(items)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            user_embeddings = layer(user_embeddings)\n",
    "            item_embeddings = layer(item_embeddings)\n",
    "            user_embeddings = self.dropout(user_embeddings)\n",
    "            item_embeddings = self.dropout(item_embeddings)\n",
    "\n",
    "        user_item_embeddings = (user_embeddings * item_embeddings).sum(dim=1)\n",
    "        return user_item_embeddings\n",
    "train_users, train_items = train[:, 0], train[:, 1]\n",
    "train_items += num_users  # Shift item indices\n",
    "edge_index = torch.tensor(np.vstack([train_users, train_items]), dtype=torch.long)\n",
    "\n",
    "embedding_size = 128\n",
    "n_layers = 3\n",
    "dropout = 0.3\n",
    "lr = 0.001\n",
    "weight_decay = 1e-5\n",
    "epochs = 100\n",
    "# batch_size = 256\n",
    "top_k = 100\n",
    "model = LightGCN(num_users, num_items, embedding_size=embedding_size, n_layers=n_layers,dropout=dropout).to(device)\n",
    "\n",
    "# model = GCNRec(num_users,num_items,embedding_size=embedding_size,n_layers=n_layers,dropout=dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# Train the model and evaluate on valid data\n",
    "best_val_loss = float('inf')\n",
    "patience = 20\n",
    "cnt = 0\n",
    "best_model_state = None\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        users, items = batch[0][:, 0].to(device), batch[0][:, 1].to(device)\n",
    "        # Generate negative samples\n",
    "        negative_items = torch.randint(0, num_items, size=(len(items),)).to(device)\n",
    "\n",
    "        positive_preds = model(users, items, edge_index.to(device))\n",
    "        negative_preds = model(users, negative_items, edge_index.to(device))\n",
    "\n",
    "        # Compute BPR loss\n",
    "        loss = -F.logsigmoid(positive_preds - negative_preds).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            user, item = batch[0][:, 0].to(device), batch[0][:, 1].to(device)\n",
    "            # Generate negative samples\n",
    "            negative_items = torch.randint(0, num_items, size=(len(items),)).to(device)\n",
    "\n",
    "            positive_preds = model(users, items, edge_index.to(device))\n",
    "            negative_preds = model(users, negative_items, edge_index.to(device))\n",
    "\n",
    "            # Compute BPR loss\n",
    "            loss = -F.logsigmoid(positive_preds - negative_preds).sum()\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {train_loss:.4f}, Validation Loss : {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(),'best_model.pth')\n",
    "        # best_model_state = model.state_dict()\n",
    "        cnt = 0\n",
    "    else:\n",
    "        cnt += 1\n",
    "    # if cnt == patience:\n",
    "    #     print(\"Early stopping\")\n",
    "    #     break\n",
    "        \n",
    "## Load the best model\n",
    "if best_model_state is not None:\n",
    "    best_model = LightGCN(num_users, num_items, embedding_size, n_layers).to(device)\n",
    "    best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "else:\n",
    "    best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test data\n",
    "test_users = test_data['user_id'].to_numpy()\n",
    "test_users = user_encoder.transform(test_users)\n",
    "test_users_tensor = torch.tensor(test_users, dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "# Make predictions for each user in the test data\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    user_embeddings, item_embeddings = best_model.user_embeddings.weight, best_model.item_embeddings.weight\n",
    "    all_scores = torch.matmul(user_embeddings[test_users], item_embeddings.t())\n",
    "    top_k_items = torch.topk(all_scores, top_k, dim=1).indices\n",
    "\n",
    "# Convert the predicted item IDs back to their original IDs\n",
    "predicted_item_ids = item_encoder.inverse_transform(top_k_items.cpu().numpy().flatten()).reshape(top_k_items.shape)\n",
    "\n",
    "# Save the predictions as a CSV file\n",
    "from tqdm import tqdm\n",
    "\n",
    "submission_data = [\n",
    "    {\"user_id\": user_id, \"item_id\": item_id}\n",
    "    for user_id, user_predictions in tqdm(enumerate(predicted_item_ids))\n",
    "    for item_id in user_predictions\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "submission = pd.DataFrame(submission_data)\n",
    "\n",
    "# Save the predictions as a CSV file\n",
    "submission.to_csv('submission_lightgcn7.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarrassingly Shallow Autoencoders for Sparse Data\n",
    "- https://arxiv.org/abs/1905.03375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(677494, 2)\n",
      "(169374, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ease_rec.model import EASE\n",
    "\n",
    "# Load and preprocess data\n",
    "data_dir = 'data/'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_data = pd.read_csv(os.path.join(data_dir, \"train_data.csv\"), names=None)\n",
    "train, valid = train_test_split(all_data, test_size=0.2, stratify=all_data['user_id'], random_state=42)\n",
    "\n",
    "test_data = pd.read_csv(os.path.join(data_dir, \"test_data.csv\"), names=None)\n",
    "\n",
    "user_list = list(all_data['user_id'].unique())\n",
    "item_list = list(all_data['item_id'].unique())\n",
    "\n",
    "num_users = len(user_list)\n",
    "num_items = len(item_list)\n",
    "num_ratings = len(all_data)\n",
    "\n",
    "user_id_dict = {old_uid: new_uid for new_uid, old_uid in enumerate(user_list)}\n",
    "all_data.user_id = [user_id_dict[x] for x in all_data.user_id.tolist()]\n",
    "\n",
    "item_id_dict = {old_uid: new_uid for new_uid, old_uid in enumerate(item_list)}\n",
    "all_data.item_id = [item_id_dict[x] for x in all_data.item_id.tolist()]\n",
    "\n",
    "train = train[['user_id', 'item_id']].to_numpy()\n",
    "valid = valid[['user_id', 'item_id']].to_numpy()\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "\n",
    "# Encode user and item IDs\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "train[:, 0] = user_encoder.fit_transform(train[:, 0])\n",
    "train[:, 1] = item_encoder.fit_transform(train[:, 1])\n",
    "\n",
    "valid[:, 0] = user_encoder.transform(valid[:, 0])\n",
    "valid[:, 1] = item_encoder.transform(valid[:, 1])\n",
    "\n",
    "# Convert train and valid data to TensorDataset\n",
    "train_dataset = TensorDataset(torch.tensor(train, dtype=torch.long))\n",
    "valid_dataset = TensorDataset(torch.tensor(valid, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18397it [00:00, 180963.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate an object of the EASE class\n",
    "ease = EASE()\n",
    "\n",
    "# Fit the EASE model on the training data\n",
    "ease.fit(all_data)\n",
    "\n",
    "# Preprocess the test data\n",
    "test_users = test_data['user_id'].to_numpy()\n",
    "test_users = user_encoder.transform(test_users)\n",
    "test_items = item_encoder.transform(item_encoder.classes_)\n",
    "test_items_tensor = torch.tensor(test_items, dtype=torch.long).to(device)\n",
    "\n",
    "# Use the EASE object to get the top-k recommendations for each user in the test data\n",
    "top_k = 20\n",
    "predictions = ease.predict(all_data, test_users, test_items, k=top_k)\n",
    "\n",
    "# Convert the predicted item IDs back to their original IDs\n",
    "predicted_item_ids = item_encoder.inverse_transform(predictions['item_id'].to_numpy())\n",
    "\n",
    "# Reshape the predicted item IDs to match the shape of the top-k items tensor\n",
    "predicted_item_ids = predicted_item_ids.reshape((len(test_users), top_k))\n",
    "from tqdm import tqdm\n",
    "# Save the predictions as a CSV file\n",
    "submission_data = [\n",
    "    {\"user_id\": user_id, \"item_id\": item_id}\n",
    "    for user_id, user_predictions in tqdm(enumerate(predicted_item_ids))\n",
    "    for item_id in user_predictions\n",
    "]\n",
    "submission = pd.DataFrame(submission_data)\n",
    "submission.to_csv('submission_ease4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8addaabea1cb039ebd0cad50c57d5e545a2dd9c4864cd769240013056fee56e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
